{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные\n",
    "\n",
    "Данные в [архиве](https://drive.google.com/file/d/15o7fdxTgndoy6K-e7g8g1M2-bOOwqZPl/view?usp=sharing). В нём два файла:\n",
    "- `news_train.txt` тестовое множество\n",
    "- `news_test.txt` тренировочное множество\n",
    "\n",
    "С некоторых новостных сайтов были загружены тексты новостей за период  несколько лет, причем каждая новость принаделжит к какой-то рубрике: `science`, `style`, `culture`, `life`, `economics`, `business`, `travel`, `forces`, `media`, `sport`.\n",
    "\n",
    "В каждой строке файла содержится метка рубрики, заголовок новостной статьи и сам текст статьи, например:\n",
    "\n",
    ">    **sport**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею разгромила чехов**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею крупно об...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача\n",
    "\n",
    "1. Обработать данные, получив для каждого текста набор токенов\n",
    "Обработать токены с помощью (один вариант из трех):\n",
    "    - pymorphy2\n",
    "    - русского [snowball стеммера](https://www.nltk.org/howto/stem.html)\n",
    "    - [SentencePiece](https://github.com/google/sentencepiece) или [Huggingface Tokenizers](https://github.com/huggingface/tokenizers)\n",
    "    \n",
    "    \n",
    "2. Обучить word embeddings (fastText, word2vec, gloVe) на тренировочных данных. Можно использовать [gensim](https://radimrehurek.com/gensim/models/word2vec.html) . Продемонстрировать семантические ассоциации. \n",
    "\n",
    "3. Реализовать алгоритм классификации документа по категориям, посчитать точноть на тестовых данных, подобрать гиперпараметры. Метод векторизации выбрать произвольно - можно использовать $tf-idf$ с понижением размерности (см. scikit-learn), можно использовать обученные на предыдущем шаге векторные представления, можно использовать [предобученные модели](https://rusvectores.org/ru/models/). Имейте ввиду, что простое \"усреднение\" токенов в тексте скорее всего не даст положительных результатов. Нужно реализовать два алгоритмов из трех:\n",
    "     - SVM\n",
    "     - наивный байесовский классификатор\n",
    "     - логистическая регрессия\n",
    "    \n",
    "\n",
    "4.* Реализуйте классификацию с помощью нейросетевых моделей. Например [RuBERT](http://docs.deeppavlov.ai/en/master/features/models/bert.html) или [ELMo](https://rusvectores.org/ru/models/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sklearn) (0.23.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.19.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (0.17.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.5.2)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1321 sha256=bf850f9e8427d6ad162beee27a6ff171bebec2c8798b59d3a45a51d8b4a05640\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\22\\0b\\40\\fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.19.2)\n",
      "Requirement already satisfied: pymorphy2 in c:\\programdata\\anaconda3\\lib\\site-packages (0.9.1)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pymorphy2) (0.7.2)\n",
      "Requirement already satisfied: docopt>=0.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pymorphy2) (0.6.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from pymorphy2) (2.4.417127.4579844)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (4.50.2)\n",
      "Requirement already satisfied: gensim in c:\\users\\admin\\appdata\\roaming\\python\\python38\\site-packages (4.1.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.5.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.19.2)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\admin\\appdata\\roaming\\python\\python38\\site-packages (from gensim) (0.29.23)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.50.2)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2020.10.15)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (0.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn\n",
    "!pip install numpy\n",
    "!pip install pymorphy2\n",
    "!pip install pandas\n",
    "!pip install tqdm\n",
    "!pip install gensim\n",
    "!pip install nltk\n",
    "\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import pymorphy2\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from scipy.stats import uniform\n",
    "from sklearn import svm, metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import FastText\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>header</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sport</td>\n",
       "      <td>Овечкин пожертвовал детской хоккейной школе ав...</td>\n",
       "      <td>Нападающий «Вашингтон Кэпиталз» Александр Овеч...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>culture</td>\n",
       "      <td>Рекордно дорогую статую майя признали подделкой</td>\n",
       "      <td>Власти Мексики объявили подделкой статую майя,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>science</td>\n",
       "      <td>Samsung представила флагман в защищенном корпусе</td>\n",
       "      <td>Южнокорейская Samsung анонсировала защищенную ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>С футболиста «Спартака» сняли четырехматчевую ...</td>\n",
       "      <td>Контрольно-дисциплинарный комитет (КДК) РФС сн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>media</td>\n",
       "      <td>Hopes &amp; Fears объединится с The Village</td>\n",
       "      <td>Интернет-издание Hopes &amp; Fears объявило о свое...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>life</td>\n",
       "      <td>Составлен рейтинг лучших европейских пляжей 20...</td>\n",
       "      <td>Опубликован рейтинг лучших европейских пляжей ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>media</td>\n",
       "      <td>В «Снобе» объяснили причину смены формата</td>\n",
       "      <td>Генеральный директор «Сноб медиа» Марина Гевор...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>economics</td>\n",
       "      <td>Минфин предложил штрафовать за биткоины на 50 ...</td>\n",
       "      <td>Минфин разработал законопроект, устанавливающи...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>life</td>\n",
       "      <td>Мэл Гибсон заплатит бывшей подруге 750 тысяч д...</td>\n",
       "      <td>Актер и режиссер Мэл Гибсон выплатит своей быв...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>media</td>\n",
       "      <td>Еще на двух линиях московского метро заработал...</td>\n",
       "      <td>На Серпуховско-Тимирязевской и Бутовской линия...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             tag                                             header  \\\n",
       "0          sport  Овечкин пожертвовал детской хоккейной школе ав...   \n",
       "1        culture    Рекордно дорогую статую майя признали подделкой   \n",
       "2        science   Samsung представила флагман в защищенном корпусе   \n",
       "3          sport  С футболиста «Спартака» сняли четырехматчевую ...   \n",
       "4          media            Hopes & Fears объединится с The Village   \n",
       "...          ...                                                ...   \n",
       "14995       life  Составлен рейтинг лучших европейских пляжей 20...   \n",
       "14996      media          В «Снобе» объяснили причину смены формата   \n",
       "14997  economics  Минфин предложил штрафовать за биткоины на 50 ...   \n",
       "14998       life  Мэл Гибсон заплатит бывшей подруге 750 тысяч д...   \n",
       "14999      media  Еще на двух линиях московского метро заработал...   \n",
       "\n",
       "                                                 content  \n",
       "0      Нападающий «Вашингтон Кэпиталз» Александр Овеч...  \n",
       "1      Власти Мексики объявили подделкой статую майя,...  \n",
       "2      Южнокорейская Samsung анонсировала защищенную ...  \n",
       "3      Контрольно-дисциплинарный комитет (КДК) РФС сн...  \n",
       "4      Интернет-издание Hopes & Fears объявило о свое...  \n",
       "...                                                  ...  \n",
       "14995  Опубликован рейтинг лучших европейских пляжей ...  \n",
       "14996  Генеральный директор «Сноб медиа» Марина Гевор...  \n",
       "14997  Минфин разработал законопроект, устанавливающи...  \n",
       "14998  Актер и режиссер Мэл Гибсон выплатит своей быв...  \n",
       "14999  На Серпуховско-Тимирязевской и Бутовской линия...  \n",
       "\n",
       "[15000 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"news_train.txt\",sep='\\t', header=None)\n",
    "train = train.rename({0:'tag', 1:'header', 2:'content'}, axis='columns')\n",
    "\n",
    "test = pd.read_csv(\"news_test.txt\",sep='\\t', header=None)\n",
    "test= test.rename({0:'tag', 1:'header', 2:'content'}, axis='columns')\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentences(df):\n",
    "    processed_texts = []\n",
    "    for i, text in tqdm(enumerate(df['content']), desc = 'Processing dataframe', total = df['content'].shape[0]):\n",
    "        text = text.lower()\n",
    "        words = re.findall(r'\\b\\w+\\b', text)\n",
    "        tokens = [morph.parse(word)[0].normal_form for word in words]\n",
    "        processed_texts.append(tokens)\n",
    "    return processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a0aa51c6cc4e0a99597b4b54bd2daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Processing dataframe'), FloatProgress(value=0.0, max=15000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99a9b4c5ce442838a3ffa9626d5af09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Processing dataframe'), FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "processed_train = process_sentences(train)\n",
    "processed_test = process_sentences(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(sentences=processed_train, window=5, min_count=1, workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12882705, 14796365)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(processed_train, total_examples=10, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('законов12', 0.9540018439292908),\n",
       " ('законно', 0.9444084167480469),\n",
       " ('законодательство', 0.9119880199432373),\n",
       " ('законцовка', 0.911150336265564),\n",
       " ('законник', 0.8937592506408691),\n",
       " ('законодательно', 0.8933196663856506),\n",
       " ('законопроект', 0.8912474513053894),\n",
       " ('закономерно', 0.8767651319503784),\n",
       " ('законодатель', 0.8725363612174988),\n",
       " ('беззаконие', 0.8713452219963074)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=['закон'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('матч', 0.7345455288887024),\n",
       " ('водапобеда', 0.7244129180908203),\n",
       " ('побег', 0.7235247492790222),\n",
       " ('матчбол', 0.7231491208076477),\n",
       " ('матчтво', 0.7160069346427917)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=['соревнование', 'победа'], negative=['кино'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_pred, y_test):\n",
    "    print(f'Precision: {precision_score(y_pred, y_test, average=\"weighted\"):.2f}')\n",
    "    print(f'Recall: {recall_score(y_pred, y_test, average=\"weighted\"):.2f}')\n",
    "    print(f'F1 score: {f1_score(y_pred, y_test, average=\"weighted\"):.2f}')\n",
    "    print(f'Accuracy: {accuracy_score(y_pred, y_test):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [' '.join(sentence) for sentence in processed_train]\n",
    "y_train = train['tag']\n",
    "X_test = [' '.join(sentence) for sentence in processed_test]\n",
    "y_test = test['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X_train_vec = tfidf.fit_transform(X_train)\n",
    "X_test_vec = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = RandomizedSearchCV(\n",
    "    LogisticRegression(),\n",
    "    param_distributions={\n",
    "        'C': np.arange(0.01, 1.01, 0.01)\n",
    "    }, \n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "naive_bayes = RandomizedSearchCV(\n",
    "    MultinomialNB(),\n",
    "    param_distributions={\n",
    "        'alpha': np.arange(0.5, 1.51, 0.01)\n",
    "    }, \n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed: 15.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=LogisticRegression(),\n",
       "                   param_distributions={'C': array([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 , 0.11,\n",
       "       0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21, 0.22,\n",
       "       0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32, 0.33,\n",
       "       0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43, 0.44,\n",
       "       0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54, 0.55,\n",
       "       0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65, 0.66,\n",
       "       0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77,\n",
       "       0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88,\n",
       "       0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99,\n",
       "       1.  ])},\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    4.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=MultinomialNB(),\n",
       "                   param_distributions={'alpha': array([0.5 , 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6 ,\n",
       "       0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7 , 0.71,\n",
       "       0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8 , 0.81, 0.82,\n",
       "       0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9 , 0.91, 0.92, 0.93,\n",
       "       0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.  , 1.01, 1.02, 1.03, 1.04,\n",
       "       1.05, 1.06, 1.07, 1.08, 1.09, 1.1 , 1.11, 1.12, 1.13, 1.14, 1.15,\n",
       "       1.16, 1.17, 1.18, 1.19, 1.2 , 1.21, 1.22, 1.23, 1.24, 1.25, 1.26,\n",
       "       1.27, 1.28, 1.29, 1.3 , 1.31, 1.32, 1.33, 1.34, 1.35, 1.36, 1.37,\n",
       "       1.38, 1.39, 1.4 , 1.41, 1.42, 1.43, 1.44, 1.45, 1.46, 1.47, 1.48,\n",
       "       1.49, 1.5 ])},\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic Regression:\n",
      "Precision: 0.89\n",
      "Recall: 0.88\n",
      "F1 score: 0.88\n",
      "Accuracy: 0.88\n",
      "\n",
      "Naive Bayes:\n",
      "Precision: 0.88\n",
      "Recall: 0.82\n",
      "F1 score: 0.85\n",
      "Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "print('logistic Regression:')\n",
    "y_pred_lreg = log_reg.best_estimator_.predict(X_test_vec)\n",
    "print_metrics(y_pred_lreg, y_test)\n",
    "\n",
    "print('\\nNaive Bayes:')\n",
    "y_pred_nb = naive_bayes.predict(X_test_vec)\n",
    "print_metrics(y_pred_nb, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почти идентичны, но логистическая регрессия чуть лучше"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
